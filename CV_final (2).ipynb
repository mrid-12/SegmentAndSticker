{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "WAHXf_ElWKzi"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "BG7sXZ_oOGf9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHiSL1sNimXz"
      },
      "outputs": [],
      "source": [
        "!pip install segment_anything\n",
        "import torch\n",
        "import torchvision\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image, ImageOps\n",
        "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "!pip install 'git+https://github.com/facebookresearch/segment-anything.git' transformers opencv-python matplotlib\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "wfep7sAbOMoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device(\"cuda\")\n",
        "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "detector  = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\").to(device)\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "\n",
        "sam.to(device=device)\n",
        "predictor = SamPredictor(sam)"
      ],
      "metadata": {
        "id": "OlRTvznX2fki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OWL ViT object detection using open prompt"
      ],
      "metadata": {
        "id": "7s6djF_vPab6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bounding_box_detection(inputimg, prompt, threshold = 0.001):\n",
        "  img_pil = Image.open(inputimg).convert(\"RGB\")\n",
        "  img_pil = ImageOps.exif_transpose(img_pil)\n",
        "  img = np.array(img_pil)\n",
        "\n",
        "  inputs = processor(text=[prompt], images=[img_pil], return_tensors=\"pt\").to(device)\n",
        "  with torch.no_grad():\n",
        "    outputs = detector(**inputs)\n",
        "\n",
        "\n",
        "  target_sizes = torch.tensor([img_pil.size[::-1]], device=device)\n",
        "  results = processor.post_process_object_detection(\n",
        "    outputs, threshold, target_sizes=target_sizes\n",
        "  )[0]\n",
        "\n",
        "  boxes = results[\"boxes\"].cpu().numpy()\n",
        "  scores = results[\"scores\"].cpu().numpy()\n",
        "\n",
        "  print(f\"Found {len(boxes)} objects\")\n",
        "  if len(boxes) == 0:\n",
        "    return -1\n",
        "  else:\n",
        "    best_idx = scores.argmax()\n",
        "    x0, y0, x1, y1 = boxes[best_idx].astype(int)\n",
        "    box = np.array([x0, y0, x1, y1])\n",
        "\n",
        "    print(f\"Selected box {box} with score {scores[best_idx]:.3f}\")\n",
        "    return box\n"
      ],
      "metadata": {
        "id": "t5qfV7DI22-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying a segmask using Meta SAM"
      ],
      "metadata": {
        "id": "S-uHLFUnTuyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_segmask(inputimg, box, imgpath):\n",
        "  if isinstance(box, int):\n",
        "    return \"noimagelikethatfound.png\"\n",
        "\n",
        "  img = cv2.imread(inputimg)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  predictor.set_image(img)\n",
        "  masks, mask_scores, _ = predictor.predict(\n",
        "      box=np.expand_dims(box, axis=0),\n",
        "      multimask_output=False\n",
        "  )\n",
        "  mask = masks[0]\n",
        "\n",
        "  alpha = (mask.astype(np.uint8) * 255)\n",
        "  rgba  = np.dstack([img, alpha])\n",
        "\n",
        "\n",
        "  imgpath = imgpath if imgpath else \"sticker.png\"\n",
        "  sticker = Image.fromarray(rgba)\n",
        "  sticker.save(imgpath)\n",
        "  print(f\"Saved â†’ {imgpath}\")\n",
        "\n",
        "  return imgpath\n"
      ],
      "metadata": {
        "id": "26rVRQFVvUll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main function (editable inputs)"
      ],
      "metadata": {
        "id": "WAHXf_ElWKzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputpath = input(\"Enter full image path\\n\")\n",
        "prompt = input(\"Enter a prompt of what to extract from the image\\n\")\n",
        "outputpath = input(\"Enter the path of the output image (optional, saves to sticker.png)\\n\")\n",
        "box = bounding_box_detection(inputpath, prompt)\n",
        "imgpath = generate_segmask(inputpath, box, outputpath)\n",
        "\n",
        "img = Image.open(imgpath)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K82cgcNkWXbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segment and Sticker\n"
      ],
      "metadata": {
        "id": "Z1gaBSg5__gD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outputs by varying OwlViT prompts vs. thresholds"
      ],
      "metadata": {
        "id": "O4_pI6FygxQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Here is an image of a car parked outside the college library that we are using as input"
      ],
      "metadata": {
        "id": "3pUiEjDmg3Lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open(\"car.jpeg\")\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ijK_EpgPg076"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Constant threshold but varying prompt specificity"
      ],
      "metadata": {
        "id": "zLJlxYT6hNwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A threshold of above 0.25 makes it so that adjectives/specifics of the object are required to successfully create a mask of the object"
      ],
      "metadata": {
        "id": "hqX3I4aEwJXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A hyper specific input is required for high thresholds, which is not ideal for prompt complacency (human tendency to shorten prompts)\n",
        "\n",
        "box = bounding_box_detection(\"car.jpeg\", \"yellow car\", 0.3)\n",
        "img1 = generate_segmask(\"car.jpeg\", box, \"\")\n",
        "\n",
        "box = bounding_box_detection(\"car.jpeg\", \"a yellow car\", 0.3)\n",
        "img2 = generate_segmask(\"car.jpeg\", box, \"\")\n",
        "\n",
        "im1 = Image.open(img1)\n",
        "im2 = Image.open(img2)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "plt.subplots_adjust(wspace=0.05)\n",
        "axs[0].imshow(im1)\n",
        "axs[0].axis('off')\n",
        "axs[1].imshow(im2)\n",
        "axs[1].axis('off')\n",
        "plt.suptitle(\"Prompting 'Yellow Car' versus 'A Yellow Car' with a threshold of 0.3\", fontsize=14, y=0.97, weight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3cTVr2Z7hYXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a constant low threshold (reduced by a factor of 10x) and it is able to capture real world references, such as 'Mustang'"
      ],
      "metadata": {
        "id": "-QTxkC6Wwmyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we reduce the threshold, but there is definitely a confidence difference (0.114 vs 0.033)\n",
        "box = bounding_box_detection(\"car.jpeg\", \"car\", 0.03)\n",
        "img1 = generate_segmask(\"car.jpeg\", box, \"\")\n",
        "\n",
        "box = bounding_box_detection(\"car.jpeg\", \"Mustang\", 0.03)\n",
        "img2 = generate_segmask(\"car.jpeg\", box, \"\")\n",
        "\n",
        "im1 = Image.open(img1)\n",
        "im2 = Image.open(img2)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "plt.subplots_adjust(wspace=0.05)\n",
        "axs[0].imshow(im1)\n",
        "axs[0].axis('off')\n",
        "axs[1].imshow(im2)\n",
        "axs[1].axis('off')\n",
        "plt.suptitle(\"Prompting 'car' versus 'Mustang' with a threshold of 0.03\", fontsize=14, y=0.97, weight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p-eOKG0nw1WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a constant low threshold still changes according to the adjective/ specificity of the prompt leading to some weird observations"
      ],
      "metadata": {
        "id": "3mdontdm0wOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Keeping prompts constant but varying the threshold (particularly for external objects)"
      ],
      "metadata": {
        "id": "_JQZZ67mqN1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Windows (background object) not detected very well due to higher thresholds"
      ],
      "metadata": {
        "id": "DaA0UiOq6ddj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "box = bounding_box_detection(\"car.jpeg\", \"many windows\", 0.1)\n",
        "img1 = generate_segmask(\"car.jpeg\", box, \"\")\n",
        "\n",
        "box2 = bounding_box_detection(\"car.jpeg\", \"many windows\", 0.01)\n",
        "img2 = generate_segmask(\"car.jpeg\", box2, \"\")\n",
        "\n",
        "box = bounding_box_detection(\"car.jpeg\", \"many windows\", 0.001)\n",
        "img3 = generate_segmask(\"car.jpeg\", box, \"\")\n",
        "\n",
        "im1 = Image.open(img1)\n",
        "im2 = Image.open(img2)\n",
        "im3 = Image.open(img3)\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
        "plt.subplots_adjust(wspace=0.05)\n",
        "axs[0].imshow(im1)\n",
        "axs[0].axis('off')\n",
        "axs[1].imshow(im2)\n",
        "axs[1].axis('off')\n",
        "axs[2].imshow(im3)\n",
        "axs[2].axis('off')\n",
        "plt.suptitle(\"Prompting 'many windows' with thresholds 0.1, 0.01, 0.001\", fontsize=14, y=0.97, weight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fiOz4bjYyGM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good case of specific prompt and a decent threshold!"
      ],
      "metadata": {
        "id": "7nVo2ASn6bFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "box = bounding_box_detection(\"car.jpeg\", \"wheelchair symbol on the road\", 0.1)\n",
        "img1 = generate_segmask(\"car.jpeg\", box, \"\")\n",
        "\n",
        "box2 = bounding_box_detection(\"car.jpeg\", \"wheelchair symbol on the road\", 0.01)\n",
        "img2 = generate_segmask(\"car.jpeg\", box2, \"\")\n",
        "\n",
        "box = bounding_box_detection(\"car.jpeg\", \"wheelchair symbol on the road\", 0.001)\n",
        "img3 = generate_segmask(\"car.jpeg\", box, \"\")\n",
        "\n",
        "im1 = Image.open(img1)\n",
        "im2 = Image.open(img2)\n",
        "im3 = Image.open(img3)\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
        "plt.subplots_adjust(wspace=0.05)\n",
        "axs[0].imshow(im1)\n",
        "axs[0].axis('off')\n",
        "axs[1].imshow(im2)\n",
        "axs[1].axis('off')\n",
        "axs[2].imshow(im3)\n",
        "axs[2].axis('off')\n",
        "plt.suptitle(\"Prompting 'wheelchair symbol...' with thresholds 0.1, 0.01, 0.001\", fontsize=14, y=0.97, weight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DEn5dPIw6uW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inability to detect two principal objects (specific to people!)"
      ],
      "metadata": {
        "id": "vW9YWxti7wW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From practice (and the above examples) we see that multiple parts of the same image seem to be detected very well using a threshold of 0.01\n",
        "\n",
        "Here's some images involving two principal subjects!"
      ],
      "metadata": {
        "id": "MaXQRzcT8E3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "im1 = Image.open(\"bros.jpg\")\n",
        "im2 = Image.open(\"children.jpg\")\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "plt.subplots_adjust(wspace=0.05)\n",
        "axs[0].imshow(im1)\n",
        "axs[0].axis('off')\n",
        "axs[1].imshow(im2)\n",
        "axs[1].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ccyyFnp8RCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "box = bounding_box_detection(\"bros.jpg\", \"both people\", 0.01)\n",
        "img1 = generate_segmask(\"bros.jpg\", box, \"a.png\")\n",
        "\n",
        "box = bounding_box_detection(\"children.jpg\", \"both children\", 0.01)\n",
        "img2 = generate_segmask(\"children.jpg\", box, \"b.png\")\n",
        "\n",
        "im1 = Image.open(img1)\n",
        "im2 = Image.open(img2)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "plt.subplots_adjust(wspace=0.05)\n",
        "axs[0].imshow(im1)\n",
        "axs[0].axis('off')\n",
        "axs[1].imshow(im2)\n",
        "axs[1].axis('off')\n",
        "plt.suptitle(\"Prompting to include both people in the image with a threshold of 0.01\", fontsize=14, y=0.97, weight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KvVt6uIZ931R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But it works perfectly fine with some carrots?"
      ],
      "metadata": {
        "id": "2sLOe4s1-qqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "box = bounding_box_detection(\"carrots.jpg\", \"carrot\", 0.01)\n",
        "img1 = generate_segmask(\"carrots.jpg\", box, \"d.png\")\n",
        "\n",
        "box = bounding_box_detection(\"carrots.jpg\", \"carrots\", 0.01)\n",
        "img2 = generate_segmask(\"carrots.jpg\", box, \"c.png\")\n",
        "\n",
        "im1 = Image.open(img1)\n",
        "im2 = Image.open(img2)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "plt.subplots_adjust(wspace=0.05)\n",
        "axs[0].imshow(im1)\n",
        "axs[0].axis('off')\n",
        "axs[1].imshow(im2)\n",
        "axs[1].axis('off')\n",
        "plt.suptitle(\"Prompting to include all objects in the image with a threshold of 0.01\", fontsize=14, y=0.97, weight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kwZHGczP-0HF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}